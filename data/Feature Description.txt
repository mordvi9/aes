1. noun_count

What it is: The total count of noun tokens (including singular and plural common nouns as well as proper nouns) detected in the essay using Part-Of-Speech (POS) tagging.
Source: Extracted from the essay text based on tags such as NN, NNS, NNP, NNPS.
Why it’s used: Noun frequency can indicate the level of detail and specificity in an essay, which may correlate with essay quality and content richness.


2.verb_count

What it is: The total count of verb tokens in the essay.
Source: Derived from POS tags like VB, VBD, VBG, VBN, VBP, and VBZ.
Why it’s used: Verb usage reflects sentence dynamics and action, contributing to grammatical correctness and the overall coherence of the essay.


3.adj_count

What it is: The count of adjectives present in the essay.
Source: Detected using POS tags such as JJ, JJR, and JJS.
Why it’s used: Adjectives add descriptive quality and nuance to the text and can indicate the writer’s ability to elaborate and paint vivid descriptions.


4.adv_count

What it is: The total number of adverbs found in the essay.
Source: Extracted from POS tags like RB, RBR, and RBS.
Why it’s used: Adverbs help convey how actions are performed, contributing to the complexity and fluidity of the writing.


5.pronoun_count

What it is: The count of pronouns used in the essay.
Source: Determined using tags PRP and PRP$.
Why it’s used: A high frequency of pronouns might indicate vague language; however, appropriate use is also a sign of natural language flow. This balance helps assess the clarity of writing.


6.modal_count

What it is: The number of modal verbs in the essay.
Source: Based on the 'MD' POS tag.
Why it’s used: Modal verbs (like can, might, should) are important for conveying possibility, probability, or necessity, impacting the tone and formality of the writing.


7.complexity_verb_ratio

What it is: The ratio of verbs per sentence (verb_count divided by the number of sentences).
Source: Computed using the verb count and the total number of sentences from the essay text.
Why it’s used: This ratio serves as a proxy for sentence complexity and dynamism; essays with a good balance may be more engaging or sophisticated.


8.adj_adv_ratio

What it is: The combined count of adjectives and adverbs divided by the total number of tokens.
Source: Uses counts of adjectives and adverbs from the essay.
Why it’s used: This ratio indicates the descriptive density of the text, which can influence readability and the perceived quality of the essay.


9.avg_sentence_length

What it is: The average number of words per sentence.
Source: Calculated using the textstat library on the essay text.
Why it’s used: Sentence length is a key readability metric; too short or too long sentences can affect the flow and comprehension of the essay.


10.avg_syllables_per_word

What it is: The average number of syllables per word in the essay.
Source: Also computed by the textstat library.
Why it’s used: This metric is used to estimate word complexity, influencing the overall readability of the text.


11. flesch_reading_ease

What it is: A readability score based on the Flesch Reading Ease formula.
Source: Provided by the textstat library using the essay text.
Why it’s used: It quantifies how easy or difficult the text is to read. Higher scores suggest more accessible writing.


12.tfidf_cosine_similarity

What it is: The cosine similarity between the TF-IDF vectors of the essay prompt and the essay.
Source: Derived from the TF-IDF vectorizer and cosine similarity function.
Why it’s used: This metric assesses the relevance of the essay to the given prompt by comparing their textual features.


13.sbert_prompt_adherence_similarity

What it is: The cosine similarity between embeddings generated by the Sentence Transformer for the prompt and the essay.
Source: Computed using the SentenceTransformer (SBERT) model.
Why it’s used: It provides a similarity measure between the prompt and essay, ensuring that the essay stays on topic.


14.mean_tfidf_score

What it is: The mean TF-IDF score across all words in the essay.
Source: Calculated from the TF-IDF scores extracted from the essay text.
Why it’s used: This represents the average importance or weight of the words used, which can be indicative of vocabulary richness.


15. std_tfidf_score

What it is: The standard deviation of the TF-IDF scores for the words in the essay.
Source: Derived from the distribution of TF-IDF scores.
Why it’s used: It indicates the variability or diversity in word importance throughout the text.


16.unique_word_ratio

What it is: The ratio of unique words to the total number of words in the essay.
Source: Calculated by extracting words using POS tagging and comparing unique counts to total counts.
Why it’s used: A higher ratio reflects lexical diversity and can be a marker of a richer vocabulary.


17.window_repeat_count

What it is: A count of repeated words within sliding windows (consecutive sentence groups) in the essay.
Source: Evaluated by sliding a window over the sentences and checking for word repetitions.
Why it’s used: This helps in identifying redundancy or overuse of certain terms, which may affect the perceived quality of the writing.


18.num_grammatical_errors
What it is: The total number of detected grammatical errors based on a set of heuristics applied to each sentence.
Source: Derived from various grammatical checks using the essay text and its POS tags. Checks include:

    Sentence without a main verb.
    Short sentences with subordinators.
    Modal verbs without a base verb.
    Subject-verb agreement issues.
    Run-on sentences or comma splices.
    Excessive use of pronouns or simple verbs.
    Prepositions at the end of sentences.
    Double modals.
    Determiners not followed by a noun/adjective.
    Duplicate consecutive words.
    Sentences starting without a capitalized word.
    Consecutive punctuation errors.

    Why it’s used: This comprehensive error count serves as an automated quality check for the essay’s grammatical structure, a critical factor in scoring.







Predict Groudtruth: Score

What it is: The target score provided for the essay, used as the label for regression.
Source: Loaded directly from the dataset.
Why it’s used: This is the outcome variable that the regression model will attempt to predict based on the extracted features.

